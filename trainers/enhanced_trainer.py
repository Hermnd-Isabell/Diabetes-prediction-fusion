#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè®­ç»ƒå™¨ - æ”¯æŒå››ä¸ªæ¨¡å‹å¹¶åŒ…å«ä¸°å¯Œçš„å¯è§†åŒ–å’Œå¯è§£é‡Šæ€§åˆ†æ

æ”¯æŒçš„æ¨¡å‹:
- AttentionMultimodal (æ³¨æ„åŠ›æœºåˆ¶)
- Baseline (ConcatFusion, EnsembleFusion)
- TFTMultimodal (æ—¶åºèåˆTransformer)

åŠŸèƒ½ç‰¹æ€§:
- å¤šæ¨¡å‹è®­ç»ƒå’Œå¯¹æ¯”
- ä¸°å¯Œçš„å¯è§†åŒ–å±•ç¤º
- å¯è§£é‡Šæ€§åˆ†æ
- æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ª
- æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
"""

import os
import json
import time
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.metrics import (
    roc_auc_score, f1_score, roc_curve, precision_recall_curve,
    accuracy_score, confusion_matrix, classification_report
)
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import shap

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')


class EnhancedTrainer:
    """
    å¢å¼ºç‰ˆè®­ç»ƒå™¨ç±»
    
    æ”¯æŒå¤šæ¨¡å‹è®­ç»ƒã€å¯è§†åŒ–åˆ†æå’Œå¯è§£é‡Šæ€§ç ”ç©¶
    """
    
    def __init__(
        self,
        model: nn.Module,
        model_name: str,
        device: str = "cpu",
        lr: float = 1e-3,
        weight_decay: float = 1e-4,
        save_dir: str = "results",
        enable_visualization: bool = True,
        enable_interpretability: bool = True
    ):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆè®­ç»ƒå™¨
        
        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            model_name: æ¨¡å‹åç§°
            device: è®­ç»ƒè®¾å¤‡
            lr: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            save_dir: ç»“æœä¿å­˜ç›®å½•
            enable_visualization: æ˜¯å¦å¯ç”¨å¯è§†åŒ–
            enable_interpretability: æ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§åˆ†æ
        """
        self.model = model.to(device)
        self.model_name = model_name
        self.device = device
        self.save_dir = Path(save_dir) / model_name
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒè®¾ç½®
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5
        )
        
        # åŠŸèƒ½å¼€å…³
        self.enable_visualization = enable_visualization
        self.enable_interpretability = enable_interpretability
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [], 'acc': [], 'auc': [], 'f1': []
        }
        self.val_history = {
            'loss': [], 'acc': [], 'auc': [], 'f1': []
        }
        
        # æœ€ä½³æ¨¡å‹
        self.best_val_auc = 0.0
        self.best_model_state = None
        
        print(f"ğŸš€ å¢å¼ºç‰ˆè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š æ¨¡å‹: {model_name}")
        print(f"ğŸ’» è®¾å¤‡: {device}")
        print(f"ğŸ’¾ ä¿å­˜ç›®å½•: {self.save_dir}")
        print(f"ğŸ“ˆ å¯è§†åŒ–: {'å¯ç”¨' if enable_visualization else 'ç¦ç”¨'}")
        print(f"ğŸ” å¯è§£é‡Šæ€§: {'å¯ç”¨' if enable_interpretability else 'ç¦ç”¨'}")
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        all_probs = []
        
        pbar = tqdm(train_loader, desc=f"è®­ç»ƒ {self.model_name}")
        for batch in pbar:
            # æ•°æ®å‡†å¤‡
            spectra = batch["spectra"].to(self.device)
            mask = batch.get("mask", None)
            if mask is not None:
                mask = mask.to(self.device)
            tabular = batch["tabular"].to(self.device)
            labels = batch["label"].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(spectra, mask, tabular)
            logits = outputs["logits"]
            loss = self.criterion(logits, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item() * labels.size(0)
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(logits, dim=1)
            
            all_preds.extend(preds.detach().cpu().numpy())
            all_labels.extend(labels.detach().cpu().numpy())
            all_probs.extend(probs[:, 1].detach().cpu().numpy())
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._calculate_metrics(all_labels, all_probs, all_preds)
        metrics['loss'] = total_loss / len(train_loader.dataset)
        
        return metrics
    
    def eval_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        all_probs = []
        all_features = []
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"éªŒè¯ {self.model_name}")
            for batch in pbar:
                # æ•°æ®å‡†å¤‡
                spectra = batch["spectra"].to(self.device)
                mask = batch.get("mask", None)
                if mask is not None:
                    mask = mask.to(self.device)
                tabular = batch["tabular"].to(self.device)
                labels = batch["label"].to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(spectra, mask, tabular)
                logits = outputs["logits"]
                loss = self.criterion(logits, labels)
                
                # ç»Ÿè®¡
                total_loss += loss.item() * labels.size(0)
                probs = torch.softmax(logits, dim=1)
                preds = torch.argmax(logits, dim=1)
                
                all_preds.extend(preds.detach().cpu().numpy())
                all_labels.extend(labels.detach().cpu().numpy())
                all_probs.extend(probs[:, 1].detach().cpu().numpy())
                
                # æ”¶é›†ç‰¹å¾ç”¨äºå¯è§†åŒ–
                if 'embedding' in outputs:
                    all_features.append(outputs['embedding'].cpu().numpy())
                
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._calculate_metrics(all_labels, all_probs, all_preds)
        metrics['loss'] = total_loss / len(val_loader.dataset)
        
        # ä¿å­˜ç‰¹å¾ç”¨äºåç»­åˆ†æ
        if all_features:
            metrics['features'] = np.vstack(all_features)
        
        return metrics
    
    def _calculate_metrics(self, y_true: List, y_prob: List, y_pred: List) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        y_true = np.array(y_true)
        y_prob = np.array(y_prob)
        y_pred = np.array(y_pred)
        
        try:
            auc = roc_auc_score(y_true, y_prob)
        except ValueError:
            auc = 0.5
        
        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='weighted')
        
        # è®¡ç®—æ•æ„Ÿæ€§@90%ç‰¹å¼‚æ€§
        fpr, tpr, _ = roc_curve(y_true, y_prob)
        specificity = 1 - fpr
        mask = specificity >= 0.9
        sens_at_90 = tpr[mask].max() if np.any(mask) else np.nan
        
        return {
            'acc': acc,
            'auc': auc,
            'f1': f1,
            'sensitivity@90%spec': sens_at_90
        }
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int = 50,
        early_stopping_patience: int = 10,
        save_best: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
            save_best: æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
        
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {self.model_name}")
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"ğŸ“Š éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {epochs}")
        print("=" * 60)
        
        start_time = time.time()
        best_epoch = 0
        patience_counter = 0
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒå’ŒéªŒè¯
            train_metrics = self.train_epoch(train_loader)
            val_metrics = self.eval_epoch(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_metrics['loss'])
            
            # è®°å½•å†å²
            for key in self.train_history:
                if key in train_metrics:
                    self.train_history[key].append(train_metrics[key])
            for key in self.val_history:
                if key in val_metrics:
                    self.val_history[key].append(val_metrics[key])
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['auc'] > self.best_val_auc:
                self.best_val_auc = val_metrics['auc']
                best_epoch = epoch
                patience_counter = 0
                if save_best:
                    self.best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            epoch_time = time.time() - epoch_start
            print(f"Epoch {epoch+1:3d}/{epochs} | "
                  f"Train: Loss={train_metrics['loss']:.4f}, "
                  f"AUC={train_metrics['auc']:.4f}, "
                  f"Acc={train_metrics['acc']:.4f} | "
                  f"Val: Loss={val_metrics['loss']:.4f}, "
                  f"AUC={val_metrics['auc']:.4f}, "
                  f"Acc={val_metrics['acc']:.4f} | "
                  f"Time={epoch_time:.1f}s")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= early_stopping_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ (patience={early_stopping_patience})")
                break
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸  æ€»æ—¶é—´: {total_time:.1f}s")
        print(f"ğŸ† æœ€ä½³éªŒè¯AUC: {self.best_val_auc:.4f} (Epoch {best_epoch+1})")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if save_best and self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            self.save_model()
        
        # ç”Ÿæˆå¯è§†åŒ–
        if self.enable_visualization:
            self._generate_training_visualizations()
        
        return {
            'best_val_auc': self.best_val_auc,
            'best_epoch': best_epoch,
            'total_time': total_time,
            'train_history': self.train_history,
            'val_history': self.val_history
        }
    
    def evaluate(
        self,
        test_loader: DataLoader,
        generate_plots: bool = True
    ) -> Dict[str, Any]:
        """
        æ¨¡å‹è¯„ä¼°
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            generate_plots: æ˜¯å¦ç”Ÿæˆè¯„ä¼°å›¾è¡¨
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\nğŸ” è¯„ä¼° {self.model_name}")
        print("=" * 40)
        
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []
        all_features = []
        all_attention_weights = []
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="è¯„ä¼°"):
                # æ•°æ®å‡†å¤‡
                spectra = batch["spectra"].to(self.device)
                mask = batch.get("mask", None)
                if mask is not None:
                    mask = mask.to(self.device)
                tabular = batch["tabular"].to(self.device)
                labels = batch["label"].to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(spectra, mask, tabular)
                logits = outputs["logits"]
                probs = torch.softmax(logits, dim=1)
                preds = torch.argmax(logits, dim=1)
                
                # æ”¶é›†ç»“æœ
                all_preds.extend(preds.detach().cpu().numpy())
                all_labels.extend(labels.detach().cpu().numpy())
                all_probs.extend(probs[:, 1].detach().cpu().numpy())
                
                # æ”¶é›†ç‰¹å¾å’Œæ³¨æ„åŠ›æƒé‡
                if 'embedding' in outputs:
                    all_features.append(outputs['embedding'].cpu().numpy())
                if 'attention_weights' in outputs:
                    all_attention_weights.append(outputs['attention_weights'].cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._calculate_metrics(all_labels, all_probs, all_preds)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = classification_report(
            all_labels, all_preds,
            target_names=['Control', 'DM'],
            output_dict=True
        )
        
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   â€¢ å‡†ç¡®ç‡: {metrics['acc']:.4f}")
        print(f"   â€¢ AUC: {metrics['auc']:.4f}")
        print(f"   â€¢ F1åˆ†æ•°: {metrics['f1']:.4f}")
        print(f"   â€¢ æ•æ„Ÿæ€§@90%ç‰¹å¼‚æ€§: {metrics['sensitivity@90%spec']:.4f}")
        
        # ç”Ÿæˆå¯è§†åŒ–
        if generate_plots and self.enable_visualization:
            # ç¡®ä¿ç‰¹å¾å’Œæ³¨æ„åŠ›æƒé‡æ˜¯numpyæ•°ç»„
            features_array = np.vstack(all_features) if all_features else None
            attention_array = np.vstack(all_attention_weights) if all_attention_weights else None
            
            self._generate_evaluation_plots(
                all_labels, all_probs, all_preds,
                features_array, attention_array
            )
        
        # å¯è§£é‡Šæ€§åˆ†æ
        if self.enable_interpretability:
            # ç¡®ä¿ç‰¹å¾å’Œæ³¨æ„åŠ›æƒé‡æ˜¯numpyæ•°ç»„
            features_array = np.vstack(all_features) if all_features else None
            attention_array = np.vstack(all_attention_weights) if all_attention_weights else None
            
            self._generate_interpretability_analysis(
                test_loader, features_array, attention_array
            )
        
        return {
            'metrics': metrics,
            'classification_report': report,
            'predictions': {
                'labels': all_labels,
                'probabilities': all_probs,
                'predictions': all_preds
            },
            'features': np.vstack(all_features) if all_features else None,
            'attention_weights': np.vstack(all_attention_weights) if all_attention_weights else None
        }
    
    def _generate_training_visualizations(self):
        """ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–"""
        print("ğŸ“Š ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'{self.model_name} - è®­ç»ƒè¿‡ç¨‹', fontsize=16, fontweight='bold')
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.train_history['loss'], label='è®­ç»ƒæŸå¤±', color='blue', alpha=0.7)
        axes[0, 0].plot(self.val_history['loss'], label='éªŒè¯æŸå¤±', color='red', alpha=0.7)
        axes[0, 0].set_title('æŸå¤±æ›²çº¿')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(self.train_history['acc'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue', alpha=0.7)
        axes[0, 1].plot(self.val_history['acc'], label='éªŒè¯å‡†ç¡®ç‡', color='red', alpha=0.7)
        axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # AUCæ›²çº¿
        axes[1, 0].plot(self.train_history['auc'], label='è®­ç»ƒAUC', color='blue', alpha=0.7)
        axes[1, 0].plot(self.val_history['auc'], label='éªŒè¯AUC', color='red', alpha=0.7)
        axes[1, 0].set_title('AUCæ›²çº¿')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('AUC')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # F1åˆ†æ•°æ›²çº¿
        axes[1, 1].plot(self.train_history['f1'], label='è®­ç»ƒF1', color='blue', alpha=0.7)
        axes[1, 1].plot(self.val_history['f1'], label='éªŒè¯F1', color='red', alpha=0.7)
        axes[1, 1].set_title('F1åˆ†æ•°æ›²çº¿')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('F1 Score')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / 'training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è®­ç»ƒå¯è§†åŒ–å·²ä¿å­˜: {self.save_dir / 'training_curves.png'}")
    
    def _generate_evaluation_plots(
        self,
        y_true: List,
        y_prob: List,
        y_pred: List,
        features: Optional[np.ndarray] = None,
        attention_weights: Optional[np.ndarray] = None
    ):
        """ç”Ÿæˆè¯„ä¼°å›¾è¡¨"""
        print("ğŸ“Š ç”Ÿæˆè¯„ä¼°å¯è§†åŒ–...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'{self.model_name} - æ¨¡å‹è¯„ä¼°', fontsize=16, fontweight='bold')
        
        # ROCæ›²çº¿
        fpr, tpr, _ = roc_curve(y_true, y_prob)
        auc = roc_auc_score(y_true, y_prob)
        axes[0, 0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {auc:.3f})')
        axes[0, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)
        axes[0, 0].set_xlim([0.0, 1.0])
        axes[0, 0].set_ylim([0.0, 1.05])
        axes[0, 0].set_xlabel('å‡æ­£ç‡ (FPR)')
        axes[0, 0].set_ylabel('çœŸæ­£ç‡ (TPR)')
        axes[0, 0].set_title('ROCæ›²çº¿')
        axes[0, 0].legend(loc="lower right")
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
        precision, recall, _ = precision_recall_curve(y_true, y_prob)
        axes[0, 1].plot(recall, precision, color='blue', lw=2)
        axes[0, 1].set_xlabel('å¬å›ç‡')
        axes[0, 1].set_ylabel('ç²¾ç¡®ç‡')
        axes[0, 1].set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
        axes[0, 1].grid(True, alpha=0.3)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 2],
                   xticklabels=['Control', 'DM'], yticklabels=['Control', 'DM'])
        axes[0, 2].set_title('æ··æ·†çŸ©é˜µ')
        axes[0, 2].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[0, 2].set_ylabel('çœŸå®æ ‡ç­¾')
        
        # é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        axes[1, 0].hist([y_prob[i] for i in range(len(y_prob)) if y_true[i] == 0],
                       bins=20, alpha=0.7, label='Control', color='blue')
        axes[1, 0].hist([y_prob[i] for i in range(len(y_prob)) if y_true[i] == 1],
                       bins=20, alpha=0.7, label='DM', color='red')
        axes[1, 0].set_xlabel('é¢„æµ‹æ¦‚ç‡')
        axes[1, 0].set_ylabel('é¢‘æ¬¡')
        axes[1, 0].set_title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # ç‰¹å¾å¯è§†åŒ– (t-SNE)
        if features is not None and len(features) > 10:
            try:
                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)//4))
                features_2d = tsne.fit_transform(features)
                
                scatter = axes[1, 1].scatter(features_2d[:, 0], features_2d[:, 1], 
                                           c=y_true, cmap='viridis', alpha=0.6)
                axes[1, 1].set_title('ç‰¹å¾ç©ºé—´å¯è§†åŒ– (t-SNE)')
                axes[1, 1].set_xlabel('t-SNE 1')
                axes[1, 1].set_ylabel('t-SNE 2')
                plt.colorbar(scatter, ax=axes[1, 1])
            except Exception as e:
                axes[1, 1].text(0.5, 0.5, f't-SNEå¤±è´¥:\n{str(e)}', 
                               ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('ç‰¹å¾ç©ºé—´å¯è§†åŒ– (å¤±è´¥)')
        else:
            axes[1, 1].text(0.5, 0.5, 'ç‰¹å¾æ•°æ®ä¸è¶³', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
            axes[1, 1].set_title('ç‰¹å¾ç©ºé—´å¯è§†åŒ–')
        
        # æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
        if attention_weights is not None and len(attention_weights) > 0:
            # æ˜¾ç¤ºå¹³å‡æ³¨æ„åŠ›æƒé‡
            avg_attention = np.mean(attention_weights, axis=0)
            if len(avg_attention.shape) == 1:
                axes[1, 2].bar(range(len(avg_attention)), avg_attention)
                axes[1, 2].set_title('å¹³å‡æ³¨æ„åŠ›æƒé‡')
                axes[1, 2].set_xlabel('ç‰¹å¾ç»´åº¦')
                axes[1, 2].set_ylabel('æ³¨æ„åŠ›æƒé‡')
            else:
                im = axes[1, 2].imshow(avg_attention, cmap='viridis', aspect='auto')
                axes[1, 2].set_title('æ³¨æ„åŠ›æƒé‡çƒ­å›¾')
                plt.colorbar(im, ax=axes[1, 2])
        else:
            axes[1, 2].text(0.5, 0.5, 'æ— æ³¨æ„åŠ›æƒé‡æ•°æ®', 
                           ha='center', va='center', transform=axes[1, 2].transAxes)
            axes[1, 2].set_title('æ³¨æ„åŠ›æƒé‡')
        
        plt.tight_layout()
        plt.savefig(self.save_dir / 'evaluation_plots.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è¯„ä¼°å¯è§†åŒ–å·²ä¿å­˜: {self.save_dir / 'evaluation_plots.png'}")
    
    def _generate_interpretability_analysis(
        self,
        test_loader: DataLoader,
        features: Optional[np.ndarray] = None,
        attention_weights: Optional[np.ndarray] = None
    ):
        """ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æ"""
        print("ğŸ” ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æ...")
        
        # è·å–ä¸€äº›æ ·æœ¬è¿›è¡ŒSHAPåˆ†æ
        sample_batch = next(iter(test_loader))
        sample_spectra = sample_batch["spectra"][:5].to(self.device)
        sample_mask = sample_batch.get("mask", None)
        if sample_mask is not None:
            sample_mask = sample_mask[:5].to(self.device)
        sample_tabular = sample_batch["tabular"][:5].to(self.device)
        
        # åˆ›å»ºç®€åŒ–çš„ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆæ›¿ä»£SHAPï¼‰
        try:
            # ä½¿ç”¨æ¢¯åº¦åˆ†ææ›¿ä»£SHAPï¼Œæ›´ç®€å•å¯é 
            single_spectra = sample_spectra[:1]  # åªåˆ†æç¬¬ä¸€ä¸ªæ ·æœ¬
            single_mask = sample_mask[:1] if sample_mask is not None else None
            single_tabular = sample_tabular[:1]
            
            # è®¡ç®—æ¢¯åº¦é‡è¦æ€§
            single_spectra.requires_grad_(True)
            outputs = self.model(single_spectra, single_mask, single_tabular)
            loss = outputs["logits"].sum()
            loss.backward()
            
            # è·å–æ¢¯åº¦ä½œä¸ºç‰¹å¾é‡è¦æ€§
            feature_importance = torch.abs(single_spectra.grad).cpu().numpy().flatten()
            
            # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
            plt.figure(figsize=(12, 8))
            plt.plot(feature_importance)
            plt.title(f'{self.model_name} - ç‰¹å¾é‡è¦æ€§ (åŸºäºæ¢¯åº¦)')
            plt.xlabel('æ³¢é•¿ç´¢å¼•')
            plt.ylabel('æ¢¯åº¦é‡è¦æ€§')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(self.save_dir / 'shap_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… ç‰¹å¾é‡è¦æ€§åˆ†æå·²ä¿å­˜: {self.save_dir / 'shap_analysis.png'}")
            
        except Exception as e:
            print(f"âš ï¸  SHAPåˆ†æå¤±è´¥: {e}")
            # å¦‚æœSHAPå¤±è´¥ï¼Œè‡³å°‘ç”Ÿæˆä¸€ä¸ªç®€å•çš„ç‰¹å¾é‡è¦æ€§å›¾
            try:
                plt.figure(figsize=(12, 8))
                # ä½¿ç”¨ç®€å•çš„ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
                feature_importance = np.abs(sample_spectra.cpu().numpy().mean(axis=0))
                plt.plot(feature_importance.mean(axis=0))
                plt.title(f'{self.model_name} - ç‰¹å¾é‡è¦æ€§ (æ›¿ä»£SHAP)')
                plt.xlabel('æ³¢é•¿ç´¢å¼•')
                plt.ylabel('å¹³å‡å¼ºåº¦')
                plt.grid(True, alpha=0.3)
                plt.tight_layout()
                plt.savefig(self.save_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')
                plt.close()
                print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {self.save_dir / 'feature_importance.png'}")
            except Exception as e2:
                print(f"âš ï¸  ç‰¹å¾é‡è¦æ€§å›¾ä¹Ÿå¤±è´¥: {e2}")
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        if features is not None:
            self._analyze_feature_importance(features)
        
        # æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
        if attention_weights is not None:
            self._analyze_attention_patterns(attention_weights)
    
    def _analyze_feature_importance(self, features: np.ndarray):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        # ä½¿ç”¨PCAåˆ†æç‰¹å¾é‡è¦æ€§
        if features.shape[0] > 1 and features.shape[1] > 1:
            # ç¡®ä¿n_componentsä¸è¶…è¿‡æ ·æœ¬æ•°å’Œç‰¹å¾æ•°
            max_components = min(10, features.shape[0] - 1, features.shape[1])
            if max_components > 0:
                pca = PCA(n_components=max_components)
                pca.fit(features)
                
                # å¯è§†åŒ–ä¸»æˆåˆ†è´¡çŒ®
                plt.figure(figsize=(10, 6))
                plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), 
                       pca.explained_variance_ratio_)
                plt.xlabel('ä¸»æˆåˆ†')
                plt.ylabel('è§£é‡Šæ–¹å·®æ¯”')
                plt.title(f'{self.model_name} - ä¸»æˆåˆ†åˆ†æ')
                plt.grid(True, alpha=0.3)
                plt.tight_layout()
                plt.savefig(self.save_dir / 'pca_analysis.png', dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"âœ… PCAåˆ†æå·²ä¿å­˜: {self.save_dir / 'pca_analysis.png'}")
            else:
                print("âš ï¸  æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œè·³è¿‡PCAåˆ†æ")
        else:
            print("âš ï¸  ç‰¹å¾æ•°æ®ä¸è¶³ï¼Œè·³è¿‡PCAåˆ†æ")
    
    def _analyze_attention_patterns(self, attention_weights: np.ndarray):
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼"""
        if len(attention_weights.shape) >= 2:
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡çš„ç»Ÿè®¡ä¿¡æ¯
            mean_attention = np.mean(attention_weights, axis=0)
            std_attention = np.std(attention_weights, axis=0)
            
            # å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼
            plt.figure(figsize=(12, 6))
            
            if len(mean_attention.shape) == 1:
                # ä¸€ç»´æ³¨æ„åŠ›æƒé‡
                plt.subplot(1, 2, 1)
                plt.bar(range(len(mean_attention)), mean_attention)
                plt.title('å¹³å‡æ³¨æ„åŠ›æƒé‡')
                plt.xlabel('ç‰¹å¾ç»´åº¦')
                plt.ylabel('æ³¨æ„åŠ›æƒé‡')
                
                plt.subplot(1, 2, 2)
                plt.bar(range(len(std_attention)), std_attention)
                plt.title('æ³¨æ„åŠ›æƒé‡æ ‡å‡†å·®')
                plt.xlabel('ç‰¹å¾ç»´åº¦')
                plt.ylabel('æ ‡å‡†å·®')
            else:
                # äºŒç»´æ³¨æ„åŠ›æƒé‡
                plt.subplot(1, 2, 1)
                plt.imshow(mean_attention, cmap='viridis', aspect='auto')
                plt.title('å¹³å‡æ³¨æ„åŠ›æƒé‡')
                plt.colorbar()
                
                plt.subplot(1, 2, 2)
                plt.imshow(std_attention, cmap='viridis', aspect='auto')
                plt.title('æ³¨æ„åŠ›æƒé‡æ ‡å‡†å·®')
                plt.colorbar()
            
            plt.tight_layout()
            plt.savefig(self.save_dir / 'attention_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… æ³¨æ„åŠ›åˆ†æå·²ä¿å­˜: {self.save_dir / 'attention_analysis.png'}")
    
    def save_model(self, filename: str = "best_model.pt"):
        """ä¿å­˜æ¨¡å‹"""
        save_path = self.save_dir / filename
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_name': self.model_name,
            'best_val_auc': self.best_val_auc,
            'train_history': self.train_history,
            'val_history': self.val_history
        }, save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    def load_model(self, filename: str = "best_model.pt"):
        """åŠ è½½æ¨¡å‹"""
        load_path = self.save_dir / filename
        if load_path.exists():
            checkpoint = torch.load(load_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.best_val_auc = checkpoint.get('best_val_auc', 0.0)
            self.train_history = checkpoint.get('train_history', {'loss': [], 'acc': [], 'auc': [], 'f1': []})
            self.val_history = checkpoint.get('val_history', {'loss': [], 'acc': [], 'auc': [], 'f1': []})
            print(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {load_path}")
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
    
    def get_model_summary(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'model_name': self.model_name,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024),  # å‡è®¾float32
            'best_val_auc': self.best_val_auc,
            'device': str(self.device)
        }


def compare_models(
    trainers: List[EnhancedTrainer],
    test_loader: DataLoader,
    save_dir: str = "results/comparison"
) -> Dict[str, Any]:
    """
    æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
    
    Args:
        trainers: è®­ç»ƒå™¨åˆ—è¡¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        save_dir: ä¿å­˜ç›®å½•
    
    Returns:
        æ¯”è¾ƒç»“æœå­—å…¸
    """
    print(f"\nğŸ”„ å¼€å§‹æ¨¡å‹æ¯”è¾ƒ...")
    print("=" * 50)
    
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    results = {}
    all_metrics = []
    
    # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    for trainer in trainers:
        print(f"\nğŸ“Š è¯„ä¼° {trainer.model_name}...")
        result = trainer.evaluate(test_loader, generate_plots=False)
        results[trainer.model_name] = result
        all_metrics.append({
            'model': trainer.model_name,
            **result['metrics']
        })
    
    # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
    metrics_df = pd.DataFrame(all_metrics)
    metrics_df = metrics_df.set_index('model')
    
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
    print(metrics_df.round(4))
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    metrics_df.to_csv(save_dir / 'model_comparison.csv')
    
    # ç”Ÿæˆæ¯”è¾ƒå¯è§†åŒ–
    _generate_comparison_plots(trainers, results, save_dir)
    
    return {
        'metrics': metrics_df,
        'detailed_results': results,
        'best_model': metrics_df['auc'].idxmax(),
        'best_auc': metrics_df['auc'].max()
    }


def _generate_comparison_plots(
    trainers: List[EnhancedTrainer],
    results: Dict[str, Any],
    save_dir: Path
):
    """ç”Ÿæˆæ¨¡å‹æ¯”è¾ƒå›¾è¡¨"""
    print("ğŸ“Š ç”Ÿæˆæ¨¡å‹æ¯”è¾ƒå¯è§†åŒ–...")
    
    # æå–æŒ‡æ ‡
    model_names = list(results.keys())
    metrics = ['acc', 'auc', 'f1', 'sensitivity@90%spec']
    
    # åˆ›å»ºæ¯”è¾ƒå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ', fontsize=16, fontweight='bold')
    
    for i, metric in enumerate(metrics):
        row, col = i // 2, i % 2
        values = [results[name]['metrics'][metric] for name in model_names]
        
        bars = axes[row, col].bar(model_names, values, alpha=0.7)
        axes[row, col].set_title(f'{metric.upper()} æ¯”è¾ƒ')
        axes[row, col].set_ylabel(metric.upper())
        axes[row, col].tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                              f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ROCæ›²çº¿æ¯”è¾ƒ
    plt.figure(figsize=(10, 8))
    for name, result in results.items():
        y_true = result['predictions']['labels']
        y_prob = result['predictions']['probabilities']
        fpr, tpr, _ = roc_curve(y_true, y_prob)
        auc = result['metrics']['auc']
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)
    
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡ (FPR)')
    plt.ylabel('çœŸæ­£ç‡ (TPR)')
    plt.title('ROCæ›²çº¿æ¯”è¾ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_dir / 'roc_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æ¯”è¾ƒå¯è§†åŒ–å·²ä¿å­˜: {save_dir}")


if __name__ == "__main__":
    print("ğŸš€ å¢å¼ºç‰ˆè®­ç»ƒå™¨æ¨¡å—")
    print("è¯·é€šè¿‡ä¸»è„šæœ¬ä½¿ç”¨æ­¤è®­ç»ƒå™¨")


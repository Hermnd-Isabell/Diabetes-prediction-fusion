#!/usr/bin/env python3
"""
å‡æ•°æ®ç”Ÿæˆå™¨ - ç”¨äºæµ‹è¯• AttentionMultimodal æ¨¡å‹
æ”¯æŒç”Ÿæˆä¸åŒè§„æ¨¡å’Œé…ç½®çš„æµ‹è¯•æ•°æ®
"""

import torch
import numpy as np
from models.attention_models import AttentionMultimodal

class FakeDataGenerator:
    """å‡æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, seed=42):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        torch.manual_seed(seed)
        np.random.seed(seed)
        self.seed = seed
    
    def generate_spectral_data(self, batch_size, num_scans=3, num_wavelengths=1000):
        """ç”Ÿæˆå…‰è°±æ•°æ®"""
        spectra = torch.randn(batch_size, num_scans, num_wavelengths)
        mask = torch.ones(batch_size, num_scans, dtype=torch.bool)
        return spectra, mask
    
    def generate_tabular_data(self, batch_size, num_features=10):
        """ç”Ÿæˆè¡¨æ ¼æ•°æ®"""
        tabular = torch.randn(batch_size, num_features)
        return tabular
    
    def generate_labels(self, batch_size, num_classes=2):
        """ç”Ÿæˆæ ‡ç­¾"""
        labels = torch.randint(0, num_classes, (batch_size,))
        return labels
    
    def generate_full_data(self, batch_size, num_classes=2, num_features=10):
        """ç”Ÿæˆå®Œæ•´çš„æ•°æ®é›†"""
        spectra, mask = self.generate_spectral_data(batch_size)
        tabular = self.generate_tabular_data(batch_size, num_features)
        labels = self.generate_labels(batch_size, num_classes)
        
        return {
            'spectra': spectra,
            'mask': mask,
            'tabular': tabular,
            'labels': labels
        }
    
    def generate_pretrained_embeddings(self, batch_size, spec_dim=256, tab_dim=128, num_classes=2):
        """ç”Ÿæˆé¢„è®¡ç®—çš„embeddingå’Œlogits"""
        spec_result = {
            'embedding': torch.randn(batch_size, spec_dim),
            'logits': torch.randn(batch_size, num_classes)
        }
        
        tab_result = {
            'embedding': torch.randn(batch_size, tab_dim),
            'logits': torch.randn(batch_size, num_classes)
        }
        
        return spec_result, tab_result

def test_with_different_configurations():
    """æµ‹è¯•ä¸åŒé…ç½®çš„æ¨¡å‹"""
    print("ğŸ”§ æµ‹è¯•ä¸åŒé…ç½®çš„æ¨¡å‹")
    print("=" * 60)
    
    generator = FakeDataGenerator()
    
    # æµ‹è¯•é…ç½®
    configs = [
        {'batch_size': 1, 'num_classes': 2, 'num_features': 5},
        {'batch_size': 4, 'num_classes': 2, 'num_features': 10},
        {'batch_size': 8, 'num_classes': 3, 'num_features': 15},
        {'batch_size': 16, 'num_classes': 2, 'num_features': 20},
    ]
    
    for i, config in enumerate(configs, 1):
        print(f"\nğŸ“Š é…ç½® {i}: {config}")
        
        # ç”Ÿæˆæ•°æ®
        data = generator.generate_full_data(**config)
        
        # åˆ›å»ºæ¨¡å‹
        model = AttentionMultimodal(
            spec_embedding_dim=256,
            tab_embedding_dim=128,
            num_classes=config['num_classes'],
            fusion_type='enhanced_cross',
            tab_dim=config['num_features']
        )
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            output = model(data['spectra'], data['mask'], data['tabular'])
            
            print(f"   âœ… æˆåŠŸ")
            print(f"   â€¢ ä¸»logitså½¢çŠ¶: {output['logits'].shape}")
            print(f"   â€¢ èåˆembeddingå½¢çŠ¶: {output['embedding'].shape}")
            print(f"   â€¢ å…‰è°±embeddingå½¢çŠ¶: {output['spec_embedding'].shape}")
            print(f"   â€¢ è¡¨æ ¼embeddingå½¢çŠ¶: {output['tab_embedding'].shape}")

def test_pretrained_embeddings():
    """æµ‹è¯•é¢„è®¡ç®—çš„embedding"""
    print("\nğŸ”— æµ‹è¯•é¢„è®¡ç®—çš„embedding")
    print("=" * 60)
    
    generator = FakeDataGenerator()
    
    # ç”Ÿæˆé¢„è®¡ç®—çš„æ•°æ®
    spec_result, tab_result = generator.generate_pretrained_embeddings(
        batch_size=4, spec_dim=256, tab_dim=128, num_classes=2
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = AttentionMultimodal(
        spec_embedding_dim=256,
        tab_embedding_dim=128,
        num_classes=2,
        fusion_type='enhanced_cross'
    )
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        output = model(spec_result, tab_result)
        
        print(f"âœ… é¢„è®¡ç®—embeddingæµ‹è¯•æˆåŠŸ")
        print(f"   â€¢ è¾“å‡ºé”®: {list(output.keys())}")
        for key, value in output.items():
            print(f"   â€¢ {key}: {value.shape}")

def test_training_loop():
    """æµ‹è¯•è®­ç»ƒå¾ªç¯"""
    print("\nğŸ‹ï¸ æµ‹è¯•è®­ç»ƒå¾ªç¯")
    print("=" * 60)
    
    generator = FakeDataGenerator()
    
    # ç”Ÿæˆæ•°æ®
    data = generator.generate_full_data(batch_size=8, num_classes=2, num_features=10)
    
    # åˆ›å»ºæ¨¡å‹
    model = AttentionMultimodal(
        spec_embedding_dim=256,
        tab_embedding_dim=128,
        num_classes=2,
        fusion_type='enhanced_cross',
        tab_dim=10
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒå‡ ä¸ªepoch
    model.train()
    for epoch in range(3):
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        output = model(data['spectra'], data['mask'], data['tabular'])
        
        # è®¡ç®—æŸå¤±
        main_loss = torch.nn.CrossEntropyLoss()(output['logits'], data['labels'])
        spec_loss = torch.nn.CrossEntropyLoss()(output['spec_logits'], data['labels'])
        tab_loss = torch.nn.CrossEntropyLoss()(output['tab_logits'], data['labels'])
        aux_spec_loss = torch.nn.CrossEntropyLoss()(output['aux_spec_logits'], data['labels'])
        aux_tab_loss = torch.nn.CrossEntropyLoss()(output['aux_tab_logits'], data['labels'])
        
        total_loss = main_loss + 0.5 * (spec_loss + tab_loss) + 0.3 * (aux_spec_loss + aux_tab_loss)
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()
        
        print(f"   Epoch {epoch+1}: æ€»æŸå¤± = {total_loss.item():.4f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ å‡æ•°æ®ç”Ÿæˆå™¨æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ä¸åŒé…ç½®
    test_with_different_configurations()
    
    # æµ‹è¯•é¢„è®¡ç®—embedding
    test_pretrained_embeddings()
    
    # æµ‹è¯•è®­ç»ƒå¾ªç¯
    test_training_loop()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    main()


